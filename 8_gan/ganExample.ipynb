{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 777\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "# Training Hyperparameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "LOG_DIR = os.path.join(ROOT_DIR, \"logs\", now.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "LOG_ITER = 100\n",
    "CKPT_DIR = os.path.join(ROOT_DIR, \"checkpoints\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not os.path.exists(CKPT_DIR):\n",
    "    os.makedirs(CKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# CIFAR-10 Dataset\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# CIFAR-10 Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 Test Dataset\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# CIFAR-10 Test Dataloader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for CIFAR-10\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 3072)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(3072)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator for CIFAR-10\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(3072, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Generator and Discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "\n",
    "# Setup Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/782 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (6144x32 and 3072x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb 셀 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m fake_data \u001b[39m=\u001b[39m generator(noise)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Train on Real Data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m d_real_output \u001b[39m=\u001b[39m discriminator(real_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m d_real_loss \u001b[39m=\u001b[39m criterion(d_real_output, real_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m d_real_loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1186\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb 셀 9\u001b[0m in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1186\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (6144x32 and 3072x1024)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training\n",
    "iteration = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(train_loader, desc=\"Epoch {}\".format(epoch))\n",
    "    for data in pbar:\n",
    "        iteration += 1\n",
    "        # Get input data\n",
    "        real_data = data[0].to(DEVICE)\n",
    "        real_labels = torch.ones(real_data.size(0), 1).to(DEVICE)\n",
    "        fake_labels = torch.zeros(real_data.size(0), 1).to(DEVICE)\n",
    "\n",
    "        # Train Discriminator\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Get Generator Input Data\n",
    "        noise = torch.randn(real_data.size(0), 100).to(DEVICE)\n",
    "        fake_data = generator(noise)\n",
    "\n",
    "        # Train on Real Data\n",
    "        d_real_output = discriminator(real_data)\n",
    "        d_real_loss = criterion(d_real_output, real_labels)\n",
    "        d_real_loss.backward()\n",
    "\n",
    "        # Train on Fake Data\n",
    "        d_fake_output = discriminator(fake_data)\n",
    "        d_fake_loss = criterion(d_fake_output, fake_labels)\n",
    "        d_fake_loss.backward()\n",
    "\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        generator.zero_grad()\n",
    "\n",
    "        # Train on Fake Data\n",
    "        g_output = discriminator(fake_data)\n",
    "        g_loss = criterion(g_output, real_labels)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Log Losses\n",
    "        writer.add_scalar(\"Discriminator Loss\", d_loss, iteration)\n",
    "        writer.add_scalar(\"Generator Loss\", g_loss, iteration)\n",
    "\n",
    "        # Log Images\n",
    "        writer.add_image(\"Real Data\", real_data[0], iteration)\n",
    "        writer.add_image(\"Fake Data\", fake_data[0], iteration)\n",
    "\n",
    "        if iteration % LOG_ITER == 0:\n",
    "            # Save Model\n",
    "            torch.save(\n",
    "                generator.state_dict(),\n",
    "                os.path.join(CKPT_DIR, \"generator-{}.pth\".format(iteration)),\n",
    "            )\n",
    "            torch.save(\n",
    "                discriminator.state_dict(),\n",
    "                os.path.join(CKPT_DIR, \"discriminator-{}.pth\".format(iteration)),\n",
    "            )\n",
    "\n",
    "            # Log Progress\n",
    "            pbar.set_description(\n",
    "                \"D: {:.4f} G: {:.4f}\".format(d_loss.item(), g_loss.item())\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1327f5ca0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/thkim/.pyenv/versions/3.9.10/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1508, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/thkim/.pyenv/versions/3.9.10/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1472, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/thkim/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/thkim/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Users/thkim/.pyenv/versions/3.9.10/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/thkim/.pyenv/versions/3.9.10/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'iteration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb 셀 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(real_data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m100\u001b[39m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m fake_data \u001b[39m=\u001b[39m generator(noise)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m writer\u001b[39m.\u001b[39madd_image(\u001b[39m\"\u001b[39m\u001b[39mReal Data\u001b[39m\u001b[39m\"\u001b[39m, real_data[\u001b[39m0\u001b[39m], iteration)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m writer\u001b[39m.\u001b[39madd_image(\u001b[39m\"\u001b[39m\u001b[39mFake Data\u001b[39m\u001b[39m\"\u001b[39m, fake_data[\u001b[39m0\u001b[39m], iteration)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thkim/Documents/GitHub/ML102/8_gan/ganExample.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iteration' is not defined"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        real_data = data[0].to(DEVICE)\n",
    "        noise = torch.randn(real_data.size(0), 100).to(DEVICE)\n",
    "        fake_data = generator(noise)\n",
    "\n",
    "        writer.add_image(\"Real Data\", real_data[0], iteration)\n",
    "        writer.add_image(\"Fake Data\", fake_data[0], iteration)\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Results\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_result(num_epoch, show=False, save=False, path=None):\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            real_data = data[0].to(DEVICE)\n",
    "            noise = torch.randn(real_data.size(0), 100).to(DEVICE)\n",
    "            fake_data = generator(noise)\n",
    "\n",
    "            img_list = [real_data[0], fake_data[0]]\n",
    "            name_list = [\"Real Data\", \"Fake Data\"]\n",
    "            for img, name in zip(img_list, name_list):\n",
    "                img = img.mul(0.5).add(0.5).cpu().numpy()\n",
    "                plt.figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('3.9.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efaaffe9068f6e7dc4e7db139e9a8ff91e5a2b37fb0a8de094ded86cab042f17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
